<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Rejection Sampling IMLE">
  <meta property="og:title" content="RS_IMLE"/>
  <meta property="og:description" content="RS-IMLE"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="RS-IMLE">
  <meta name="twitter:description" content="Rejection Sampling IMLE">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>RS-IMLE</title>
  <link rel="icon" type="image/x-icon" href="static/images/logo.svg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<style>
  .green-square {
    width: 10px;  /* size of the square */
    height: 10px;
    background-color: green;
  }
</style>


<style>
  .orange-dot {
    width: 10px;  /* diameter of the dot */
    height: 10px;
    background-color: orange;
    border-radius: 50%;  /* makes it circular */
  }
</style>



<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Rejection Sampling IMLE: Designing Priors for Better Few-Shot Image Synthesis</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://serchirag.github.io/" target="_blank">Chirag Vashist</a>,</span>
                <span class="author-block">
                  <a href="https://sites.google.com/view/niopeng/home" target="_blank">Shichong Peng</a>,</span>
                  <span class="author-block">
                    <a href="https://www.sfu.ca/~keli/" target="_blank">Ke Li</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Simon Fraser University<br>European Conference on Computer Vision (ECCV) 2024</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2409.17439" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="https://github.com/SerChirag/rs-imle" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://www.youtube.com/watch?v=TIXEiqfm-Bw" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2409.17439" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <span class="hero-span-body">
        <video poster="" id="gcat-interpolate" autoplay muted loop width="32%">
          <!-- Your video here -->
          <source src="static/videos/gcat.mp4" type="video/mp4">
        </video>
        <video poster="" id="ffhq-interpolate" autoplay muted loop width="32%">
          <!-- Your video here -->
          <source src="static/videos/ffhq.mp4" type="video/mp4">
        </video>

        <video poster="" id="panda-interpolate" autoplay muted loop width="32%">
          <!-- Your video here -->
          <source src="static/videos/panda.mp4" type="video/mp4">
        </video>
        <!-- <video poster="" id="anime-interpolate" autoplay muted loop width="22%">
          <source src="static/videos/anime.mp4" type="video/mp4">
        </video> -->
        <!-- <video poster="" id="anime-interpolate" autoplay muted loop height="50%">
          <source src="static/videos/anime.mp4" type="video/mp4">
        </video> -->
      </span>
      <br>
      <br>
      <h2 class="subtitle has-text-justified">
        <b>TLDR:</b> We identity an issue with the current IMLE-based methods and propose a novel approach to address it.
        We achieve <b><i>state-of-the-art performance</i></b> on few-shot image generation tasks.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            An emerging area of research aims to learn deep generative models with limited training data. 
            Implicit Maximum Likelihood Estimation (IMLE), a recent technique, successfully addresses the mode collapse issue of GANs and has been adapted to the few-shot setting, achieving state-of-the-art performance. 
            However, current IMLE-based approaches encounter challenges due to inadequate correspondence between the latent codes selected for training and those drawn during inference. 
            This results in suboptimal test-time performance. We theoretically show a way to address this issue and propose RS-IMLE, a novel approach that changes the prior distribution used for training. 
            This leads to substantially higher quality image generation compared to existing GAN and IMLE-based methods, as validated by comprehensive experiments conducted on nine few-shot image datasets.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Youtube video -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Why is Few-Shot Generation Challenging?</h2>
          <div class="content has-text-justified">
            Generative models that perform well in the large-scale setting, do not perform well in the few-shot setting. 
          </div>
          <h3 class="title is-4" style="text-align: left;">Diffusion models</h3>
          <div class="content has-text-justified">
            In diffusion models, the marginal likelihood under the forward process is a mixture of isotropic Gaussians. This modeling assumption smooths out the learned manifold along all directions, including those that are orthogonal to the actual data manifold.
            This becomes particularly problematic when there are a limited number of training examples. <br>
            Suppose we have a dataset of 10K examples. We can train a diffusion model on this dataset and sample from it. 
        </div>
      </div>
    </div>
  </section>

  <div class="container">
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <span class="hero-span-body" style="display: flex; justify-content: center;">
          <div style="max-width: 40%;">
            <img src="static/videos/diffusion/infi_10000.png" width="80%">
            <p class="text-description">Dataset with 10K points</p>
          </div>
          <div style="max-width: 40%;">
            <video poster="" id="rsimle-video" autoplay muted controls loop width="80%">
              <source src="static/videos/diffusion/sine_generation_10000.mp4" type="video/mp4">
            </video>
            <p class="text-description">Denoising process for 10K data points</p>
        </span>
      </div>
    </div>
  </div>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="content has-text-justified">
              Consider what happens when we have the same shape but with just 20 data points. Now we train the <i><b>same diffusion model</b></i> on this dataset. <br>
              The performance of diffusion models degrades significantly in the few-shot setting. The model is not be able to learn the data manifold and generates samples that are far from the data manifold.
            </div>
            </div>
          </div>
        </div>
      </div>
    </section>

  <div class="container">
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <span class="hero-span-body" style="display: flex; justify-content: center;">
          <div style="max-width: 40%;">
            <img src="static/videos/diffusion/infi_20.png" width="80%">
            <p class="text-description">Dataset with 20 points</p>
          </div>
          <div style="max-width: 40%;">
            <video poster="" id="rsimle-video" autoplay muted loop controls width="80%">
              <source src="static/videos/diffusion/sine_generation_20.mp4" type="video/mp4">
            </video>
            <p class="text-description">Denoising process for 20 data points</p>
        </span>
      </div>
    </div>
  </div>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h3 class="title is-4" style="text-align: left;">Generative Adversarial Network</h3>
            <div class="content has-text-justified">
              GANs suffer from mode collapse and training instability. When mode collapse occurs, the modelled distribution only covers a subset of the modes. <br>
              Implicit Maximum Likelihood Estimation is an alternative to the GAN objective and has shown promising results in addressing mode collapse.
            </div>
          </div>
        </div>
      </div>
    </section>

    <div class="container">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <span class="hero-span-body" style="display: flex; justify-content: center;">
            <div style="max-width: 40%;">
              <video poster="" id="imle-video" autoplay muted controls loop width="80%">
                <source src="static/videos/gan.mp4" type="video/mp4">
              </video>
              <p class="text-description">GAN training: mode collapse</p>
            </div>
            <div style="max-width: 40%;">
              <video poster="" id="rsimle-video" autoplay muted controls loop width="80%">
                <source src="static/videos/imle_training.mp4" type="video/mp4">
              </video>
              <p class="text-description">IMLE training</p>
          </span>
        </div>
      </div>
    </div>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">The Misalignment Issue</h2>
          <div class="content has-text-justified">
            In the existing IMLE-based methods, we observe that the distributions of the latent codes used for training the objective differs from the distribution of latent encountered at test time. 
            Consider an illustrative example where the latent space is two dimensional. 
            We train a simple generative model using IMLE on two dimensional toy dataset. The latent codes used for training over the course of training are illustrated below.
          </div>
        </div>
      </div>
    </div>
  </section>

<div class="container">
  <!-- Paper video. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <span class="hero-span-body" style="display: flex; justify-content: center;">
        <div style="max-width: 40%;">
          <img src="static/images/compare/latent_coloured-1950.png" width="80%">
          <p class="text-description">Latent space of model trained by IMLE</p>
        </div>
        <div style="max-width: 40%;">
          <img src="static/images/compare/normal.png" width="80%">
          <p class="text-description">Latent codes sampled at inference</p>
      </span>
    </div>
  </div>
</div>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            We notice that for the latent codes belonging to the same data point (denoted by the same colour) form well-separated tight bands in the latent space. We also observe that there are <b>large gaps</b> between these bands, indicating that these segments of the latent space are consistently overlooked during training. <br>
            Since at test time we sample from the same standard normal distribution, these unsupervised segments in the latent space have arbitrary outputs, which result in bad samples.
          </div>
        </div>
      </div>
    </div>
  </section>


    


  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Methodology</h2>
          <div class="content has-text-justified">            
            In our approach, we propose to change the prior distribution used for training the model.
            We design a prior such that all samples we obtain by using the prior are guaranteed to be <i>some</i> distance (&epsi;) away from all data points. <br>
            Since we reject samples that are too close to the data points, we call our method Rejection Sampling IMLE (RS-IMLE). <br>
            <b>Note:</b> ðŸŸ© denotes data points and ðŸŸ  denote samples from the model. <br>

          </div>
        </div>
      </div>
    </div>

    <div class="container"></div>
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <span class="hero-span-body" style="display: flex; justify-content: center;">
            <div style="max-width: 40%;">
              <img src="static/images/compare/epoch-500.png" width="80%">
              <p class="text-description">IMLE</p>
            </div>
            <div style="max-width: 40%;">
              <img src="static/images/compare/improved-epoch-500.png" width="80%">
              <p class="text-description">RS-IMLE</p>
          </span>
        </div>
      </div>
    </div>

    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            Our method ensures that the loss for each data point is always sufficiently high (indicated by the long arrows), resulting in meaningful updates to the model parameters. 
            Here is a video of the training process of the models trained by IMLE and RS-IMLE.
          </div>
        </div>
      </div>
    </div>

    <div class="container">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <span class="hero-span-body" style="display: flex; justify-content: center;">
            <div style="max-width: 40%;">
              <video poster="" id="imle-video" autoplay muted controls loop width="80%">
                <source src="static/videos/imle_training.mp4" type="video/mp4">
              </video>
              <p class="text-description">IMLE</p>
            </div>
            <div style="max-width: 40%;">
              <video poster="" id="rsimle-video" autoplay muted controls loop width="80%">
                <source src="static/videos/rsimle_training-0.09.mp4" type="video/mp4">
              </video>
              <p class="text-description">RS-IMLE</p>
          </span>
        </div>
      </div>
    </div>

    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            We can also analyze the latent space of model trained by the respective objectives.
            We observe that for our method over the course of training, samples latent codes that follow the distribution at test time more faithfully.
          </div>
        </div>
      </div>
    </div>
    <br>
    <br>
    <div class="container"></div>
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <span class="hero-span-body" style="display: flex; justify-content: center;">
            <div style="max-width: 40%;">
              <video poster="" id="imle-latent-video" autoplay muted loop width="90%">
                <source src="static/videos/imle_training_latent.mp4" type="video/mp4">
              </video>
              <p class="text-description">Latent space of IMLE</p>
            </div>
            <div style="max-width: 40%;">
              <video poster="" id="rsimle-latent-video" autoplay muted loop width="90%">
                <source src="static/videos/rsimle_training_latent-0.09.mp4" type="video/mp4">
              </video>
              <p class="text-description">Latent space of RS-IMLE</p>
            </div>
          </span>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small"></section>
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Paper video. -->
      
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Frechet Inception Distance</h2>
          <div class="content has-text-justified">
            <p>
              We present the FID scores computed for all the datasets across different methods. Lower FID scores indicates that the distribution of generated images is closer to the distribution of real images. 
              <br>
              Our method performs significantly better compared to baselines. We report an average improvement of <b>45.9%</b> over the best baseline.

              <br>
              <br>
            </p>
          
          
            <style type="text/css">
              .tg  {border-collapse:collapse;border-spacing:0;}
              .tg .tg-j6zm{font-weight:bold;}
              </style>
              <table class="tg"><thead>
                <tr>
                  <th class="tg-0thz">Dataset</th>
                  <th class="tg-za14">FastGAN</th>
                  <th class="tg-za14">FakeCLR</th>
                  <th class="tg-7zrl">FreGAN</th>
                  <th class="tg-7zrl">ReGAN</th>
                  <th class="tg-7zrl">AdaIMLE</th>
                  <th class="tg-7zrl">RS-IMLE</th>
                </tr></thead>
              <tbody>
                <tr>
                  <td class="tg-za14">Obama</td>
                  <td class="tg-za14">41.1</td>
                  <td class="tg-za14">29.9</td>
                  <td class="tg-7zrl">33.4</td>
                  <td class="tg-7zrl">45.7</td>
                  <td class="tg-7zrl">25.0</td>
                  <td class="tg-j6zm">14</td>
                </tr>
                <tr>
                  <td class="tg-za14">Grumpy Cat</td>
                  <td class="tg-za14">26.6</td>
                  <td class="tg-za14">20.6</td>
                  <td class="tg-7zrl">24.9</td>
                  <td class="tg-7zrl">27.3</td>
                  <td class="tg-7zrl">19.1</td>
                  <td class="tg-j6zm">11.5</td>
                </tr>
                <tr>
                  <td class="tg-za14">Panda</td>
                  <td class="tg-za14">10.0</td>
                  <td class="tg-za14">8.8</td>
                  <td class="tg-7zrl">9.0</td>
                  <td class="tg-7zrl">12.6</td>
                  <td class="tg-7zrl">7.6</td>
                  <td class="tg-j6zm">3.5</td>
                </tr>
                <tr>
                  <td class="tg-za14">FFHQ-100</td>
                  <td class="tg-za14">54.2</td>
                  <td class="tg-za14">62.1</td>
                  <td class="tg-7zrl">50.5</td>
                  <td class="tg-7zrl">87.4</td>
                  <td class="tg-7zrl">33.2</td>
                  <td class="tg-j6zm">12.9</td>
                </tr>
                <tr>
                  <td class="tg-za14">Cat</td>
                  <td class="tg-za14">35.1</td>
                  <td class="tg-za14">27.4</td>
                  <td class="tg-7zrl">31.0</td>
                  <td class="tg-7zrl">42.1</td>
                  <td class="tg-7zrl">24.9</td>
                  <td class="tg-j6zm">15.9</td>
                </tr>
                <tr>
                  <td class="tg-za14">Dog</td>
                  <td class="tg-za14">50.7</td>
                  <td class="tg-za14">44.4</td>
                  <td class="tg-7zrl">47.9</td>
                  <td class="tg-7zrl">57.2</td>
                  <td class="tg-7zrl">43.0</td>
                  <td class="tg-j6zm">23.1</td>
                </tr>
                <tr>
                  <td class="tg-za14">Anime</td>
                  <td class="tg-za14">69.8</td>
                  <td class="tg-za14">77.7</td>
                  <td class="tg-7zrl">59.8</td>
                  <td class="tg-7zrl">110.8</td>
                  <td class="tg-7zrl">65.8</td>
                  <td class="tg-j6zm">35.8</td>
                </tr>
                <tr>
                  <td class="tg-za14">Skulls</td>
                  <td class="tg-za14">109.6</td>
                  <td class="tg-za14">106.5</td>
                  <td class="tg-7zrl">163.3</td>
                  <td class="tg-7zrl">130.7</td>
                  <td class="tg-7zrl">81.9</td>
                  <td class="tg-j6zm">51.1</td>
                </tr>
                <tr>
                  <td class="tg-za14">Shells</td>
                  <td class="tg-za14">120.9</td>
                  <td class="tg-za14">148.4</td>
                  <td class="tg-7zrl">169.3</td>
                  <td class="tg-7zrl">236.1</td>
                  <td class="tg-7zrl">108.5</td>
                  <td class="tg-j6zm">55.4</td>
                </tr>
              </tbody></table>
            </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small"></section>
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Visual Recall Test</h2>

          <div class="content has-text-justified">
            <p>
              First column is the query image from the dataset.
              Subsequent columns are the samples produced by different methods that are closest to the query image in LPIPS feature space. <br>    
              Note that the samples from our method are:           
            </p>
            <ol>
              <li>Realistic (indicating high precision)</li>
              <li>Closer to the query (indicating high recall)</li>
              <li>Diverse (indicating that the model is not overfitting)</li>
            </ol> 
          </div>
        </div>
      </div>
    </div>      
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <!-- Image carousel -->
        <section class="hero is-small">
          <div class="hero-body">
            <div class="container">
              <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <!-- Your image here -->
                <img src="static/images/queries/query-7.drawio.png" alt="MY ALT TEXT"/>
                <h2 class="subtitle has-text-centered">
                  Anime dataset
                </h2>
              </div>
              <div class="item">
                <!-- Your image here -->
                <img src="static/images/queries/query-16.drawio.png" alt="MY ALT TEXT"/>
                <h2 class="subtitle has-text-centered">
                  Cat dataset
                </h2>
              </div>
              <div class="item">
                <!-- Your image here -->
                <img src="static/images/queries/query-33.drawio.png" alt="MY ALT TEXT"/>
                <h2 class="subtitle has-text-centered">
                  FFHQ-100 dataset
                </h2>
              </div>
          </div>
        </div>
        </div>
        </section>
        <!-- End image carousel -->
      </div>
    </div>
    <div class="container is-max-desktop">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p>
              You can find more examples in the main paper and the supplementary material.
            </p>
          </div>
        </div>
      </div>
    </div>  
  </div>
</section>

<section class="hero is-small is-centered">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video Presentation</h2>
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/TIXEiqfm-Bw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
</section>


<!-- Paper poster -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{vashist2024rejectionsamplingimledesigning,
        title={Rejection Sampling IMLE: Designing Priors for Better Few-Shot Image Synthesis}, 
        author={Chirag Vashist and Shichong Peng and Ke Li},
        year={2024},
        eprint={2409.17439},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2409.17439}, 
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
